\chapter{Conclusion}\label{chap:4}
\begin{quote}
``And as the ends and ultimates of all things accord in some mean and measure with their inceptions and originals, that same multiplicit concordance which leads forth growth from birth accomplishing by a retrogressive metamorphosis that minishing and ablation towards the final which is agreeable unto nature so is it with our subsolar being.'' \\
--- James Joyce, \emph{Ulysses}
\end{quote}

This thesis presented two case studies.
In the first, we uncovered the nature of skimming through the lens of AZDWM inspectors.
We also discussed potential pitfalls for the fledgling empiricist.
The second case study was the design of a crowd-sourced skimmer detection application.
In this chapter, we addressed details of the Android API as well as system design.
Many of the features which made Bluetana practical in the field were not noted in the original paper.
The next two sections address the broader epistemological lessons of this work.

\section{On Crowdsourced Application Design}

There is simply too much information in this world for one individual to percieve and collect at one time; thus, we are forced to collaborate in a space of other individuals for the accomplishment of tasks crucial to the achievement of knowledge. The implementation of applications for crowdsourced data collection requires the careful design of robust mechanisms for ensuring correct operation in cases of remote updates and mechanisms for streaming data. While challenging, the development of such applciations will allow us to solve the otherwise impossible problems of a massively parallel and distributed reality. 

\section{On Effective Data Analysis from Heterogeneous Sources}\label{sec:effective-data-analysis}

The second observation is that the information contained within this world is non-homogenous, and avoids rigorous classification, on account of the complexities manifest from underlying, simple deterministic systems. The ability to develop systems which can leverage these heterogenous data sources in an automated and effective manner is a non-trivial next step in our capabilities for knowledge. This work has provided some information on mechanisms for bootstrapping our current analytic capabilities, such as a nearly religeous dedication to descriptive statistics. As time progresses, these statistics will hopefully become an automated step in the systems we build, so that the collection of a core understanding of the multidimensional features of the every day at a large scale will become possible.

In the end, we care to gain knowledge of reality, a system expertfully designed to have nuance: describable by simple axioms, ``I am", but containing unimaginable complexity. The development of effective crowdsourced applications and effective data analysis techniques will allow us to reveal the simple paradigms governing this nuance while not discarding that from which it arises.
